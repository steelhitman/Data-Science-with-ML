{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from numpy import absolute\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import Sequential\n",
    "import keras\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = pd.read_csv('Training_values.csv')\n",
    "labels = pd.read_csv('Training_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([labels,values], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,~df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33 categorical 4 continuous\n",
    "\n",
    "date_recorded  -  object\n",
    "funder  -  object\n",
    "installer  -  object\n",
    "wpt_name  -  object\n",
    "num_private  -  int64\n",
    "basin  -  object\n",
    "subvillage  -  object\n",
    "region  -  object\n",
    "region_code  -  int64\n",
    "district_code  -  int64\n",
    "lga  -  object\n",
    "ward  -  object\n",
    "public_meeting  -  object\n",
    "recorded_by  -  object\n",
    "scheme_management  -  object\n",
    "scheme_name  -  object\n",
    "permit  -  object\n",
    "construction_year  -  int64\n",
    "extraction_type  -  object\n",
    "extraction_type_group  -  object\n",
    "extraction_type_class  -  object\n",
    "management  -  object\n",
    "management_group  -  object\n",
    "payment  -  object\n",
    "payment_type  -  object\n",
    "water_quality  -  object\n",
    "quality_group  -  object\n",
    "quantity  -  object\n",
    "quantity_group  -  object\n",
    "source  -  object\n",
    "source_type  -  object\n",
    "source_class  -  object\n",
    "waterpoint_type  -  object\n",
    "waterpoint_type_group  -  object\n",
    "status_group  -  object\n",
    "\n",
    "id  -  int64\n",
    "amount_tsh  -  float64\n",
    "longitude  -  float64\n",
    "latitude  -  float64\n",
    "population  -  int64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    print(\"Preprocessing Data\")\n",
    "    df = df.drop_duplicates()\n",
    "    #for column in df.columns:\n",
    "    #    print(column,\" - \", df[column].dtype)\n",
    "    df.isna().sum()\n",
    "    #df.dropna(inplace=True)\n",
    "    df.isna().sum()\n",
    "\n",
    "    drop_columns = [\"id\", \"date_recorded\", \"funder\", \"installer\", \"longitude\",\n",
    "                    \"latitude\", \"wpt_name\", \"region\", \"region_code\",\n",
    "                    \"district_code\", \"lga\", \"ward\", \"recorded_by\",\n",
    "                    \"scheme_management\", \"scheme_name\", \"construction_year\", \"extraction_type\",\n",
    "                    \"extraction_type_group\", \"extraction_type_class\", \"management\", \"management_group\",\n",
    "                    \"payment\", \"payment_type\", \"waterpoint_type\", \"waterpoint_type_group\", \"subvillage\"]\n",
    "\n",
    "\n",
    "    df.drop(drop_columns, inplace=True, axis=1)\n",
    "    df.drop('num_private', inplace=True, axis=1)\n",
    "    \n",
    "    #for columns in df.columns:\n",
    "    #    print(columns, \" - \", df[columns].dtype)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_fill(df):\n",
    "    print(\"Using Forward Fill to handle missing data\")\n",
    "    return df.ffill(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_fill(df):\n",
    "    print(\"Using Backward Fill to handle missing data\")\n",
    "    return df.bfill(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleImputer(df):\n",
    "    print(\"Using Simple Imputer to handle missing data\")\n",
    "\n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent', add_indicator=True)\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            df[column] = imputer.fit_transform(df[column].values.reshape(-1, 1))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KnnImputer(df):\n",
    "    print(\"Using KNN Imputer to handle missing data\")\n",
    "    imputer = KNNImputer(missing_values=np.nan,n_neighbors=5, add_indicator=True)\n",
    "    for column in df.columns:\n",
    "        df[column] = imputer.fit_transform(df[column].values.reshape(-1, 1))\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualisation(df):\n",
    "    print(\"Visualising data\")\n",
    "    df.plot(kind='box', subplots = True, layout = (24,5), figsize = (30,40))\n",
    "    \n",
    "    df.plot(kind='scatter', subplots=True, layout=(24, 5), figsize=(30, 40))\n",
    "    \n",
    "    df = df.drop(df[(np.absolute(df['latitude']) <= 1) &\n",
    "                 (np.absolute(df['longitude']) <= 1)].index)\n",
    "    \n",
    "    map_img = plt.imread('map.jpg')\n",
    "\n",
    "\n",
    "    plot = sns.scatterplot(data=df, y='longitude', x='latitude',\n",
    "                        hue='status_group', palette='colorblind', alpha=0.6, zorder=2)\n",
    "\n",
    "    plot.imshow(map_img,\n",
    "                aspect=plot.get_aspect(),\n",
    "                extent=plot.get_xlim() + plot.get_ylim(),\n",
    "                zorder=1)\n",
    "\n",
    "    sns.countplot(data = df, x = 'status_group')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncoding(df):\n",
    "    print(\"Using One Hot Encoding to handle string Data\")\n",
    "    \n",
    "    df_new = df.copy()\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    df_new['status_group'] = encoder.fit_transform(df['status_group'].values)\n",
    "    y = np_utils.to_categorical(df_new['status_group'])\n",
    "    \n",
    "    oneHot = OneHotEncoder(categories='auto', sparse=False, dtype=int, handle_unknown='ignore')\n",
    "    columns = []\n",
    "    for column in df.columns:\n",
    "        if str(df_new[column].dtype) == \"object\":\n",
    "            columns.append(column)\n",
    "\n",
    "    print(columns)\n",
    "        \n",
    "    oneHot_encoded = pd.DataFrame(oneHot.fit_transform(df_new[columns]), index=df_new.index, columns=oneHot.get_feature_names(df_new[columns].columns))\n",
    "    df_new.drop(columns, inplace = True, axis = 1)\n",
    "    df_new = pd.concat([df_new,oneHot_encoded], axis = 1)\n",
    "    \n",
    "    return df_new, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelEncoding(df):\n",
    "    print(\"Using Label Encoder to handle string data\")\n",
    "\n",
    "    encoder_1 = LabelEncoder()\n",
    "    label_columns = ['quantity_group', 'quality_group']\n",
    "\n",
    "    df_new = df.copy()\n",
    "\n",
    "    labels = {}\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column in label_columns:\n",
    "            df_new[column] = encoder_1.fit_transform(df[column].values)\n",
    "            labels[column] = encoder_1.classes_\n",
    "\n",
    "    graph = sns.barplot(data=df_new, x='quantity_group',\n",
    "                        y='quality_group', hue='status_group')\n",
    "    graph.set_xticklabels(labels['quantity_group'])\n",
    "    graph.set_yticklabels(labels['quality_group'])\n",
    "\n",
    "    encoder = LabelEncoder()\n",
    "    #label_columns = ['basin','public_health','permit','water_quality','quantity','quantity_group','quality','quality_group','source','source_type','source_class']\n",
    "\n",
    "    df['status_group'] = encoder.fit_transform(df['status_group'].values)\n",
    "    y = np_utils.to_categorical(df['status_group'])\n",
    "\n",
    "    for column in df.columns:\n",
    "        if str(df[column].dtype) == \"object\":\n",
    "            df[column] = encoder.fit_transform(df[column].values)\n",
    "    \n",
    "    df.head(10)\n",
    "    \n",
    "    return df, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_pearson(df):\n",
    "    sns.heatmap(df.corr('pearson'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_spearman(df):\n",
    "    sns.heatmap(df.corr('spearman'), cmap='crest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.heatmap(data=df_new, x='quantity_group', y='quality_group', hue='status_group')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minMaxScaler(df):\n",
    "    print(\"Using Min Max Scaler to Normalise the continuous data\")\n",
    "\n",
    "    normalize_columns = [\"amount_tsh\", \"gps_height\", \"population\"]\n",
    "\n",
    "    for column in normalize_columns:\n",
    "        df[column] = (df[column] - min(df[column])) / (max(df[column]) - min(df[column]))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardScaler(df):\n",
    "    print(\"Using Standard Scaler to standardise the data\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    normalize_columns = [\"amount_tsh\", \"gps_height\", \"population\"]\n",
    "    \n",
    "    scaled_df = scaler.fit_transform(df[normalize_columns].to_numpy())\n",
    "    scaled_df = pd.DataFrame(scaled_df, columns=normalize_columns, index= df.index)\n",
    "    df = df.drop(normalize_columns, axis = 1)\n",
    "    df = pd.concat([df, scaled_df], axis=1)  \n",
    "    return df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_correlation(df):\n",
    "    print(\"Using Pearson Correlation for feature selection\")\n",
    "    \n",
    "    sns.heatmap(df.corr('pearson'))\n",
    "\n",
    "    correlations = df.corr('pearson')\n",
    "\n",
    "    corr_df = correlations[-1:]\n",
    "\n",
    "    print(corr_df)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(120, 1))\n",
    "    sns.heatmap(corr_df, annot=True, ax=ax)\n",
    "    \n",
    "    threshold = 0.01\n",
    "\n",
    "    columns = list(corr_df.columns.values)\n",
    "    row = corr_df.iloc[0]\n",
    "\n",
    "    #test_columns = []\n",
    "\n",
    "    #for column in columns:\n",
    "    #    if absolute(corr_df.iloc[0][column]) < threshold:\n",
    "    #        df.drop([column], axis=1, inplace=True)\n",
    "    #    else:\n",
    "    #        test_columns.append(column)\n",
    "    #print(test_columns)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_importance(df):\n",
    "    print(\"Using Random Forest for feature importance\")\n",
    "\n",
    "    X = df.drop('status_group', axis=1)\n",
    "    y = df['status_group']\n",
    "\n",
    "    feature_names = [f\"feature {i}\" for i in range(X.shape[1])]\n",
    "    forest = RandomForestClassifier(random_state=0)\n",
    "    forest.fit(X, y)\n",
    "    \n",
    "    importances = forest.feature_importances_\n",
    "    print(importances)\n",
    "    print(feature_names)    \n",
    "    threshold = 0.01\n",
    "\n",
    "    columns = list(corr_df.columns.values)\n",
    "    row = corr_df.iloc[0]\n",
    "\n",
    "    #test_columns = []\n",
    "\n",
    "    #for column in columns:\n",
    "    #    if absolute(corr_df.iloc[0][column]) < threshold:\n",
    "    #        df.drop([column], axis=1, inplace=True)\n",
    "    #    else:\n",
    "    #        test_columns.append(column)\n",
    "    #print(test_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "#feature_selection_importance(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_feature_selection(df):\n",
    "    X = df.drop('status_group', axis=1)\n",
    "    y = df['status_group']\n",
    "    rfe = RFE(xgb.XGBClassifier(objective='multi:softprob'),n_features_to_select=42)\n",
    "    rfe = rfe.fit(X,y)\n",
    "    print(rfe.support_)\n",
    "    model = xgb.XGBClassifier(objective='multi:softprob')\n",
    "    #pipeline = Pipeline(steps = [('s', rfe),('m', model)])\n",
    "    #cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats=3 , random_state= 2)\n",
    "    #n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv = cv, n_jobs = -1, error_score='raise')\n",
    "    print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "    support = pipeline.support_\n",
    "    feature_names = pipeline.named_steps['features'].get_feature_names()\n",
    "    np.array(feature_names)[support]\n",
    "    \n",
    "    \n",
    "df_new = preprocessing(df)\n",
    "df_new, y = oneHotEncoding(df_new)\n",
    "df_new = KnnImputer(df_new)\n",
    "recursive_feature_selection(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_model(X_train, X_test, y_train, y_test):\n",
    "    print(\"Sequential Model\")\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, activation='sigmoid', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(500, activation='sigmoid'))\n",
    "    model.add(Dense(250, activation='sigmoid'))\n",
    "    model.add(Dense(200, activation='sigmoid'))\n",
    "    model.add(Dense(100, activation='sigmoid'))\n",
    "    model.add(Dense(50, activation='sigmoid'))\n",
    "    model.add(Dense(25, activation='sigmoid'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    es = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                    mode='min',\n",
    "                                    patience=10,\n",
    "                                    restore_best_weights=True)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=500, shuffle=True, verbose=2)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    for y in y_pred:\n",
    "        for i in range(len(y)):\n",
    "            if y[i] > 0.5:\n",
    "                y[i] = 1\n",
    "            else:\n",
    "                y[i] = 0\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XGBoost(X_train, X_test, y_train, y_test):\n",
    "    print(\"XGBoost Model\")\n",
    "    model = xgb.XGBClassifier(objective = 'multi:softprob', random_state = 1, n_estimators = 200, max_depth = 5)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(y_pred)\n",
    "    \n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdaBoost(X_train, X_test, y_train, y_test):\n",
    "    print(\"ADABoost Model\")\n",
    "    parameters = {\n",
    "        'n_estimators':[i for i in range(100,1000,100)],\n",
    "        'learning_rate':[i*0.1 for i in range(1,10,1)]\n",
    "    }\n",
    "    model = AdaBoostClassifier(random_state=0)\n",
    "    ada = GridSearchCV(model, parameters)\n",
    "    ada.fit(X_train, y_train)\n",
    "    y_pred = ada.predict(X_test)\n",
    "    print(y_pred)\n",
    "    print(ada.cv_results_)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradientBoost(X_train, X_test, y_train, y_test):\n",
    "    print(\"Gradient Boost Model\")\n",
    "    parameters = {\n",
    "        'n_estimators': [i for i in range(100, 1000, 100)],\n",
    "        'learning_rate': [i*0.1 for i in range(1, 10, 1)],\n",
    "        'criterion': ['friedman_mse', 'squared_error']\n",
    "    }\n",
    "    model = GradientBoostingClassifier()\n",
    "    gb = GridSearchCV(model, parameters)\n",
    "    gb.fit(X_train, y_train)\n",
    "    y_pred = gb.predict(X_test)\n",
    "    print(y_pred)\n",
    "    print(gb.cv_results_)\n",
    "\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Report(y_test, y_pred):\n",
    "    print(\"Generating Report\")\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    labels = ['0', '1', '2']\n",
    "\n",
    "    report = classification_report(y_test, y_pred, target_names=labels)\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualisation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Approach_0(df):\n",
    "    df_new = preprocessing(df)\n",
    "    \n",
    "    df_new_label_encoded = labelEncoding(df_new)\n",
    "    df_new_hot_encoded = oneHotEncoding(df_new)\n",
    "    \n",
    "    \n",
    "    \n",
    "    missing_handle = [forward_fill(df_new), backward_fill(df_new), simpleImputer(df_new), KnnImputer(df_new)]\n",
    "    encoders = [labelEncoding(df_new), oneHotEncoding(df_new)]\n",
    "    scalers = [minMaxScaler(df_new), standardScaler(df_new)]\n",
    "    feature_selection = [feature_selection_correlation(df_new), recursive_feature_selection(df_new)]\n",
    "    models = [XGBoost(X_train, X_test, y_train, y_test),\n",
    "              GradientBoost(X_train, X_test, y_train, y_test), AdaBoost(X_train, X_test, y_train, y_test), sequential_model(X_train, X_test, y_train, y_test)]\n",
    "    \n",
    "Approach_0(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Approach_1(df):\n",
    "    df = preprocessing(df)\n",
    "    df = simpleImputer(df)\n",
    "    df, y = labelEncoding(df)\n",
    "    df = minMaxScaler(df)\n",
    "    df = feature_selection_correlation(df)\n",
    "    \n",
    "    X = df.drop('status_group', axis=1)\n",
    "    y = df['status_group']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = True)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    y_pred = XGBoost(X_train, X_test, y_train, y_test)\n",
    "    report = Report(y_test, y_pred)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Approach_2(df):\n",
    "    df = preprocessing(df)\n",
    "    df = simpleImputer(df)\n",
    "    df, y = labelEncoding(df)\n",
    "    df = minMaxScaler(df)\n",
    "    df = feature_selection_correlation(df)\n",
    "\n",
    "    X = df.drop('status_group', axis=1)\n",
    "    y = df['status_group']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    y_pred = AdaBoost(X_train, X_test, y_train, y_test)\n",
    "    report = Report(y_test, y_pred)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Approach_3(df):\n",
    "    df = preprocessing(df)\n",
    "    df = simpleImputer(df)\n",
    "    df, y = labelEncoding(df)\n",
    "    df = minMaxScaler(df)\n",
    "    df = feature_selection_correlation(df)\n",
    "\n",
    "    X = df.drop('status_group', axis=1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    y_pred = sequential_model(X_train, X_test, y_train, y_test)\n",
    "    report = Report(y_test, y_pred)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Approach_4(df):\n",
    "    df = preprocessing(df)\n",
    "    df = simpleImputer(df)\n",
    "    df, y = labelEncoding(df)\n",
    "    df = minMaxScaler(df)\n",
    "    df = feature_selection_correlation(df)\n",
    "\n",
    "    X = df.drop('status_group', axis=1)\n",
    "    y = df['status_group']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=True)\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(X_test.shape)\n",
    "    print(y_test.shape)\n",
    "    y_pred = GradientBoost(X_train, X_test, y_train, y_test)\n",
    "    report = Report(y_test, y_pred)\n",
    "    print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Approach_1(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Approach_2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Approach_3(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Approach_4(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = np.array(X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.70      0.81      0.75      6350\n",
    "           1       0.54      0.09      0.15       890\n",
    "           2       0.75      0.57      0.64      4640\n",
    "\n",
    "   micro avg       0.71      0.66      0.69     11880\n",
    "   macro avg       0.66      0.49      0.52     11880\n",
    "weighted avg       0.71      0.66      0.67     11880\n",
    " samples avg       0.66      0.66      0.66     11880"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.70      0.82      0.76      6404\n",
    "           1       0.49      0.09      0.15       844\n",
    "           2       0.77      0.52      0.63      4632\n",
    "\n",
    "   micro avg       0.72      0.65      0.68     11880\n",
    "   macro avg       0.65      0.48      0.51     11880\n",
    "weighted avg       0.71      0.65      0.66     11880\n",
    " samples avg       0.65      0.65      0.65     11880"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.72      0.82      0.77      6469\n",
    "           1       0.40      0.11      0.18       839\n",
    "           2       0.77      0.59      0.67      4572\n",
    "\n",
    "   micro avg       0.73      0.68      0.70     11880\n",
    "   macro avg       0.63      0.51      0.54     11880\n",
    "weighted avg       0.72      0.68      0.69     11880\n",
    " samples avg       0.68      0.68      0.68     11880"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c69d141fca959081254a0b2e02e55787369c1159b7ddbc81e1232bfadcd3a9e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
